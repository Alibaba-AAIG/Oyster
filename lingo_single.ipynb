{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b166fff-a8f8-437a-8d99-3987dd0082f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install dashscope\n",
    "! pip install simhash\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from utils.QwQ import QWQ\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from players.user import *\n",
    "from players.developer import *\n",
    "from lingo_bp import LingoBP\n",
    "from utils.utils import *\n",
    "\n",
    "class Llm:\n",
    "    def __init__(self, q):\n",
    "        self.retries = 0\n",
    "        self.max_retries = 5\n",
    "        self.q = q\n",
    "    def query(self, inputs):\n",
    "        while self.retries <= self.max_retries:\n",
    "            try:\n",
    "                self.result = self.q.talk(*inputs)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                self.retries += 1\n",
    "                if self.retries > self.max_retries:\n",
    "                    print(f\"任务失败，达到最大重试次数{self.max_retries}\")\n",
    "                    result = (None,None)\n",
    "                    self.result = result\n",
    "                    break\n",
    "                print(f\"任务发生异常{e}，正在重试第{self.retries}次...\")\n",
    "                time.sleep(3)\n",
    "                \n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ca2ba",
   "metadata": {},
   "source": [
    "Note: \n",
    "1. 请在下方输入您的token\n",
    "    Enter your token\n",
    "2. 完成相应模型api的调用代码 [./utils/QwQ.py]\n",
    "    Complete the corresponding model api call code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866971c8-f7a3-4414-af22-c69f9a65a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.QwQ import QWQ\n",
    "TOKEN = \"sk-5b55bea36606459e861ec9f95d849a87\"\n",
    "Model = \"qwen-plus\" \n",
    "q = QWQ(\n",
    "    token=TOKEN,\n",
    "    model_type=Model\n",
    ")\n",
    "llm = Llm(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6481c4eb-b710-4f30-aa1a-8571ce6f9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 其他函数 \"\"\"\n",
    "def white_check_and_rewrite(think, language):\n",
    "    print(\"#######################white_check_and_rewrite#########################\")\n",
    "    think = think.replace('user需求分析', '用户需求分析')\n",
    "    json_blocks = re.findall(r\"```json(.*?)```\", think.replace('\\n', ''), re.DOTALL)\n",
    "    if not json_blocks:\n",
    "        raise ValueError(\"No JSON block found\")\n",
    "    parsed_json = json.loads(json_blocks[-1])\n",
    "    think = parsed_json.get('thinking', {})\n",
    "    if language == \"cn\":\n",
    "        if (\"无风险\" in think['风险意图分析'] ) or (think['风险意图分析'] == \"\") or (think['安全准则响应'] == \"\"):\n",
    "            new_think = {\n",
    "                \"用户需求分析\": think['用户需求分析'],\n",
    "                \"回复策略制定\":  think['回复策略制定']['有用性策略'] + \";\" + think['回复策略制定']['安全策略']\n",
    "                }\n",
    "        print(new_think)\n",
    "        return new_think\n",
    "    else:\n",
    "        if (\"No risk\" in think['Risk Intent Analysis'] ) or (think['Risk Intent Analysis'] == \"\") or (think['Safety Guidelines'] == \"\"):\n",
    "            new_think = {\n",
    "                \"User Needs Analysis\": think['User Needs Analysis'],\n",
    "                \"Response Strategy Formulation\":  think['Response Strategy Formulation']['Usefulness Strategy'] + \";\" + think['Response Strategy Formulation']['Safety Strategy']\n",
    "                }\n",
    "            print(new_think)\n",
    "            return new_think\n",
    "    \n",
    "    return think\n",
    "\n",
    "def get_judge_results(query, response, risk_type, risk_intention, side):\n",
    "    u = Userv3(q)\n",
    "    d = Developerv2(q)\n",
    "    if side == 'user':\n",
    "        user_judge = u.user_preference_judge(query, response)\n",
    "        return user_judge, 'None'\n",
    "    elif side == 'developer':\n",
    "        safe_judge = d.developer_preference_judge(query, response, risk_type)\n",
    "        return 'None',safe_judge\n",
    "    elif side == 'both':\n",
    "        user_judge = u.user_preference_judge(query, response)\n",
    "        safe_judge = d.developer_preference_judge(query, response, risk_type)\n",
    "        return user_judge,safe_judge\n",
    "    else:\n",
    "        print('side must be user/developer/both')\n",
    "\n",
    "def get_judge_score(user_info, developer_info):\n",
    "    try:\n",
    "        user_judge = user_info[\"user_judge\"]\n",
    "    except:\n",
    "        user_judge = user_info\n",
    "    try:\n",
    "        safe_judge = developer_info[\"safe_judge\"]\n",
    "    except:\n",
    "        safe_judge = developer_info\n",
    "    return user_judge, safe_judge\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca6ebbe-0220-4516-9cc9-ee8427403052",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" initialize及optimize主体函数 \"\"\"\n",
    "\n",
    "def initialize(input_query, Initial_think, Initial_response, language):\n",
    "    prompts_dir = os.path.join(\"./prompts\", language)\n",
    "    with open(os.path.join(prompts_dir,  'risk_analysis.txt'),'r') as file:\n",
    "        risk_analysis_sys_prompt = file.read()\n",
    "    with open(os.path.join(prompts_dir, 'safety_chain_extract.txt'), 'r') as file:\n",
    "        safety_chain_extract_sys_prompt = file.read()\n",
    "    with open(os.path.join(prompts_dir, 'safety_chain_combine.txt'), 'r') as file:\n",
    "        safety_chain_recombine_sys_prompt = file.read()\n",
    "    \n",
    "    if stage_inference:\n",
    "        output_data = llm.query([\"\", input_query])\n",
    "        Initial_think = output_data[0]\n",
    "        Initial_response = output_data[1]\n",
    "        print(\"!!!stage_inference完成!!!\")\n",
    "    print(f\"Initial_think:{Initial_think}\")\n",
    "    print(f\"Initial_response:{Initial_response}\")\n",
    "    if stage_safety_chain_extract:\n",
    "        if Initial_think == \"\" or Initial_response == \"\":\n",
    "            raise(\"该数据尚未完成前置初始化 ！！！\")\n",
    "        output_data = llm.query([safety_chain_extract_sys_prompt, Initial_think])\n",
    "        think = output_data[0]\n",
    "        response = output_data[1]\n",
    "        pattern = r'```json\\s*(.*?)\\s*```' \n",
    "        try:\n",
    "            safety_chain_matches = re.findall(pattern, response, re.DOTALL) \n",
    "            safety_chain_dict = json.loads(safety_chain_matches[0])\n",
    "            safety_chain = safety_chain_dict[\"safety_chain\"]\n",
    "        except:\n",
    "            safety_chain =  response\n",
    "        print(\"!!!stage_safety_chain_extract完成!!!\")\n",
    "    if stage_safety_chain_recombine:\n",
    "        if Initial_think == \"\" or Initial_response == \"\":\n",
    "            raise(\"该数据尚未完成前置初始化 ！！！\")\n",
    "    \n",
    "        thinking = Initial_think\n",
    "        try :\n",
    "            thinking = white_check_and_rewrite(thinking, LANGUAGE)\n",
    "        except:\n",
    "            pass\n",
    "        with open(\"./prompts/cn/structured_chain_recombine.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            safety_chain_recombine_sys_prompt = f.read()\n",
    "        safety_chain_recombine_sys_prompt = safety_chain_recombine_sys_prompt.replace(\"{{chain}}\", str(safety_chain))\n",
    "        output_data = llm.query([safety_chain_recombine_sys_prompt, input_query])\n",
    "        Safety_chain_recombine_think = output_data[0]\n",
    "        Safety_chain_recombine_response = output_data[1]\n",
    "        print(\"!!!stage_safety_chain_recombine完成!!!\")\n",
    "\n",
    "    return Initial_think, Initial_response, safety_chain, Safety_chain_recombine_think, Safety_chain_recombine_response\n",
    "\n",
    "\n",
    "def optimize(risk_type, risk_intention, query, chain, think, response, language, iterations = ['both','both']):\n",
    "    lingobp = LingoBP(q)\n",
    "    ###  INI \n",
    "    user_info, developer_info = get_judge_results(query, response, risk_type,risk_intention, iterations[0])\n",
    "    user_judge, safe_judge = get_judge_score(user_info, developer_info)\n",
    "    optimize_history = [ {\n",
    "                'query': query,\n",
    "                \"chain\":chain,\n",
    "                \"think\":think,\n",
    "                \"response\":response,\n",
    "                \"user_judge\":copy.deepcopy(user_info),\n",
    "                \"safe_judge\":copy.deepcopy(developer_info),}] \n",
    "\n",
    "    ###  Iter 0 -> N   \n",
    "    for i in tqdm(range(len(iterations))):\n",
    "        if (i !=  0) and opt_pruning:\n",
    "            if (iterations[i] == 'both') and (safe_judge['score'] == 1) and( user_judge['score'] == 1):\n",
    "                break\n",
    "            elif iterations[i] == 'user' and user_judge['score'] == 1:\n",
    "                break\n",
    "            elif iterations[i] == 'developer' and safe_judge['score'] == 1:\n",
    "                break\n",
    "        new_chain = lingobp.bp_and_update(query,chain, response, user_judge, safe_judge, language, iterations[i])\n",
    "        general_chain, new_think,new_response = combine_chain(q,query,new_chain, language)\n",
    "        chain = copy.deepcopy(new_chain)\n",
    "        think = copy.deepcopy(new_think)\n",
    "        response = copy.deepcopy(new_response)\n",
    "        user_info, developer_info = get_judge_results(query, response, risk_type,risk_intention,iterations[i])\n",
    "        user_judge, safe_judge = get_judge_score(user_info, developer_info)\n",
    "\n",
    "        optimize_history.append({\n",
    "                          'query': query,\n",
    "                          \"chain\":chain,\n",
    "                          \"general_chain\": general_chain if general_chain!=new_chain else \"\",\n",
    "                          \"think\":think,\n",
    "                          \"response\":response,\n",
    "                          \"user_judge\":copy.deepcopy(user_info),\n",
    "                          \"safe_judge\":copy.deepcopy(developer_info)\n",
    "        })\n",
    "    return optimize_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc346e8b-2791-48fd-af5a-7cbf155b9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 输入参数 \"\"\"\n",
    "stage_inference = True\n",
    "stage_risk_analysis = True\n",
    "stage_safety_chain_extract = True\n",
    "stage_safety_chain_recombine = True\n",
    "iterations = ['both','both']\n",
    "opt_pruning = True\n",
    "\n",
    "\n",
    "input_query = \"如何制作蛋糕\"\n",
    "language = 'cn'    # en/cn\n",
    "Initial_think = \"\"\n",
    "Initial_response = \"\"\n",
    "\n",
    "Risk_type = ''\n",
    "Risk_intent = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e7b9c-a7ac-400f-9146-2ea252ba0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initial_think, Initial_response, safety_chain, Safety_chain_recombine_think, Safety_chain_recombine_response = initialize(input_query, Initial_think, Initial_response, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf026a6-f606-45c9-a369-10eb2eb348cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_history = optimize(Risk_type, Risk_intent, input_query, safety_chain, Initial_think, Initial_response, language, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2552e-fb9e-4575-a0b1-8811fe152c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for history in optimize_history:\n",
    "    print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeed714-bc0e-4a6f-bb03-c0ca46e7fc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
